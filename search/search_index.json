{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"whats up?","text":"<p>I write about a mix of AI algorithms, Open-source frameworks and softwares, Personal work, and applying LLMs. I'll do my best to share the most interesting stuff including my own writing, thoughts, and experiences. I am a Machine Learning Engineer, Open-source contributor and ML and Data Science educator.  Connect with me on X and Linkedin</p>"},{"location":"#get-in-touch","title":"Get in touch","text":"<p>Do you need help operationalizing ML or Computer Vision or Large Language Models? Get in touch.</p>"},{"location":"contact/","title":"Contact","text":"<p>I contribute in open-source AI framworks for the betterment of community. If you're interested in working together please take a look at my page. I'm also available for engagements. Feel free to reach out me via email, or via X, or via Linkedin.</p>"},{"location":"contributions/","title":"Contributions","text":"<p>Here are some of my open-source contributions and integrations listed, Get in touch with me for contributing into your project or if you are starting any ambitious project.</p> <p>Vector Recipes LanceDB</p> GitHub - lancedb/vectordb-recipes: High quality resources &amp; applications for LLMs, multi-modal models and VectorDBs High quality resources &amp; applications for LLMs, multi-modal models and VectorDBs https://github.com/lancedb/vectordb-recipes https://github.com/lancedb/vectordb-recipes <p>Pandas AI</p> LanceDB Integration by PrashantDixit0 \u00b7 Pull Request #1319 \u00b7 Sinaptik-AI/pandas-ai PandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time and effort when working with data. https://github.com/Sinaptik-AI/pandas-ai/pull/1319 https://github.com/Sinaptik-AI/pandas-ai/pull/1319 <p>DSPy</p> Lancedb Integration by PrashantDixit0 \u00b7 Pull Request #1444 \u00b7 stanfordnlp/dspy DSPy: The framework for programming\u2014not prompting\u2014foundation models https://github.com/stanfordnlp/dspy/pull/1444 https://github.com/stanfordnlp/dspy/pull/1444 <p>Codium AI PRAgent</p> LanceDB Integration by PrashantDixit0 \u00b7 Pull Request #548 \u00b7 Codium-ai/pr-agent \ud83d\ude80CodiumAI PR-Agent: An AI-Powered \ud83e\udd16 Tool for Automated Pull Request Analysis, Feedback, Suggestions and More! \ud83d\udcbb\ud83d\udd0d https://github.com/Codium-ai/pr-agent/pull/548 https://github.com/Codium-ai/pr-agent/pull/548 <p>MemGPT</p> Lancedb by PrashantDixit0 \u00b7 Pull Request #455 \u00b7 cpacker/MemGPT Create LLM agents with long-term memory and custom tools \ud83d\udcda\ud83e\udd99 https://github.com/cpacker/MemGPT/pull/455 https://github.com/cpacker/MemGPT/pull/455 <p>Ultralytics - Explorer API</p> Added APIs for label count in  Explorer by PrashantDixit0 \u00b7 Pull Request #7842 \u00b7 ultralytics/ultralytics YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks. https://github.com/ultralytics/ultralytics/pull/7842 https://github.com/ultralytics/ultralytics/pull/7842"},{"location":"writing/","title":"Writing","text":"<p>I focus on writing high quality content about algorithms and product of companies with illustrative diagrams for making it easy. My goal is to make content easy to understand in simple language with diagrams, with the hope that eventually, impressions and impact will align. I hope you find something valuable. If you want me to write about your product or develop examples related to your product, reach out to me over X, Linkedin, or E-mail me.</p> <p>For RAG or LLM related posts, You can check out the categories labels in the side bar.</p>"},{"location":"writing/#videos","title":"Videos","text":"<p>\ud83e\udd17 Movie Genre Prediction with Abhishek Thakur</p>"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/","title":"HNSW: Vector search for High Dimensional datasets","text":"<p>Approximate Nearest Neighbor (ANN) search is a method for finding data points near a given point in a dataset, though not always the exact nearest one. HNSW is one of the most accurate and fastest Approximate Nearest Neighbour search algorithms, It\u2019s beneficial in high-dimensional spaces where finding the same nearest neighbor would be too slow and costly. </p> <p>There are three main types of ANN search algorithms:</p> <ol> <li>Tree-based search algorithms: Use a tree structure to organize and store data points.</li> <li>Hash-based search algorithms: Use a hash table to store and manage data points.</li> <li>Graph-based search algorithms: Use a graph structure to store data points, which can be a bit complex. HNSW is a graph-based algorithm, that will be broken down into smaller parts to make it easier to understand how it works.</li> </ol> <p>All graph-based search algorithms rely on the idea of a proximity graph, where the graph is built based on the proximity of data points, measured by their Euclidean distances. Jumping straight to HNSW might be complicated, so first, we'll explain two important algorithms that help understand HNSW: the Skip List and Navigable Small World (NSW) Graphs, which are the predecessors of HNSW.</p>"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/#skip-list","title":"Skip List","text":"<p>Skip List allows fast search capabilities similar to a sorted array but it will enable quick new element insertion, unlike sorted arrays.  In simple words, the Skip list is a multilayer linked list that stores a sorted list of elements where the top layer has links that skip over many nodes, while each lower layer skips over fewer nodes.</p> <p>To search in the skip list, we start at the top layer with the longest skips and move right. If you find that the current node\u2019s key is greater than the one you're looking for, move down to the next level and continue searching.</p> Skip list, a Probabilistic data structure <p>In the image above, the blue nodes represent the nodes that are compared to find our target value of 17, and the yellow node represents the found target.</p> <p>HNSW also uses a similar layered structure, with high-degree nodes in the top layers for quick searches and low-degree nodes in the lower layers for more accurate searches.</p>"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/#navigable-small-world-nsw-graphs","title":"Navigable Small World (NSW) Graphs","text":"<p>The main concept behind NSW graphs is that if we take proximity graphs with long-range and short-range connections, search times can be very fast.</p> <p>Here's how it works:</p> <ol> <li>Each vertex/node connects to several others, forming a \"friend list\" containing nodes connected to it making search faster.</li> <li>To search, we start at a set entry point and move to the closest neighbor.</li> <li>We continue moving to the nearest neighbor in each friend list until we can't find a closer one.</li> </ol> <p>This method makes finding a target much quicker.</p> Search for the nearest neighbor of the query vector in the NSW graph <p>Routing or Navigation through the graph happens in two steps:</p> <ol> <li>Zoom-Out Phase: We first move through vertices/nodes with fewer connections to cover a lot of ground quickly, this step makes it faster.</li> <li>Zoom-In Phase: Next, we move through vertices with more connections to get closer to our target.</li> </ol> <p>We stop when no closer vertices are in the current vertices's friend list.</p> Routing through NSW graph with high-degree and low-degree vertex"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/#hnsw-hierarchical-navigable-small-worlds","title":"HNSW: Hierarchical Navigable Small Worlds","text":"<p>HNSW improved the NSW algorithm by adding a hierarchical structure using a skip list. Adding hierarchy to NSW forms a layer graph where at the top layer, we have the longest links, and at the bottom layer, we have the shortest.</p> HNSW graph, the top layer with an entry point and the longest links, and as you go down each layer, the links get shorter <p>During the search in HNSW, we</p> <ol> <li>Start at the top layer with the longest links, these vertices usually have longer connections and spread across layers.</li> <li>Move to the nearest higher-degree vertices in each layer until reaching a local minimum.</li> <li>Switch to a lower layer and repeat the process.</li> <li>Continue this until the local minimum in the bottom layer (layer 0) is found.</li> </ol> Searching the nearest neighbor of the query vector in the HNSW graph"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/#putting-it-all-together","title":"Putting it all together","text":"<p>We can combine the above concepts to understand how to build and query an HNSW index in LanceDB.</p> <p>Construct index</p> <pre><code>import lancedb\nimport numpy as np\nuri = \"/tmp/lancedb\"\ndb = lancedb.connect(uri)\n\n# Create 10,000 sample vectors\ndata = [{\"vector\": row, \"item\": f\"item {i}\"}\n   for i, row in enumerate(np.random.random((10_000, 1536)).astype('float32'))]\n\n# Add the vectors to a table\ntbl = db.create_table(\"my_vectors\", data=data)\n\n# Create and train the HNSW index for a 1536-dimensional vector\n# Make sure you have enough data in the table for an effective training step\ntbl.create_index(index_type=IVF_HNSW_SQ)\n</code></pre> <p>Query the index <pre><code># Search using a random 1536-dimensional embedding\ntbl.search(np.random.random((1536))) \\\n    .limit(2) \\\n    .to_pandas()\n</code></pre></p> <p>You've now understood an HNSW index and how to use it in LanceDB. Next, check out how to create and use an IVF-PQ index, and explore other types of ANN indexes available in LanceDB.</p>"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/#conclusion","title":"Conclusion","text":"<p>The article discussed the Hierarchical Navigable Small World (HNSW) graph, which is a popular method for quick and accurate ANN search in high-dimensional datasets. HNSW combines two techniques: Navigable Small World graphs and layered Skip lists. When using HNSW with LanceDB, you just need to specify <code>index_type</code> and use the <code>create_index</code> function.</p>"},{"location":"writing/2024/08/23/HNSW-vector-search-for-high-dimensional-datasets/#references","title":"References","text":"<ol> <li>Skip lists: a probabilistic alternative to balanced trees (1990) by William Pugh.</li> <li>Approximate nearest neighbor algorithm based on navigable small world graphs (2014) by Y. Malkov et al.</li> <li>Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs(2016) by Y. Malkov and D. Yashunin.</li> <li>Header-only C++/python library for fast approximate nearest neighbors.</li> </ol>"},{"location":"writing/2024/08/23/IVF_PQ-accelerate-vector-search-by-creating-indices/","title":"IVFPQ: Accelerate vector search by creating indices","text":"<p>Vector similarity search is finding similar vectors from a list of given vectors in a particular embedding space. It plays a vital role in various fields and applications because it efficiently retrieves relevant information from large datasets.</p> <p>Vector similarity search requires excessive memory resources for efficient search, especially when dealing with dense vector datasets. Here comes the role of compressing High Dimensional vectors for optimizing memory storage. In this blog, We\u2019ll discuss about</p> <ol> <li>Product Quantization(PQ) &amp; How it works</li> <li>Inverted File Product Quantization(IVFPQ) Index</li> <li>Implementation of IVFPQ using LanceDB</li> </ol> <p>We\u2019ll also see the performance of PQ and IVFPQ in terms of memory and cover an implementation of the IVFPQ Index using LanceDB.</p> <p>Quantization is a process used for dimensional reduction without losing important information.</p> Quantization: Dimensionality Reduction"},{"location":"writing/2024/08/23/IVF_PQ-accelerate-vector-search-by-creating-indices/#how-does-product-quantization-work","title":"How does Product Quantization work?","text":"<p>Product Quantization can be broken down into steps listed below:</p> <ol> <li>Divide a large, high-dimensional vector into equally sized chunks, creating subvectors.</li> <li>Identify the nearest centroid for each subvector, referring to it as reproduction or reconstruction values.</li> <li>Replace these reproduction values with unique IDs that represent the corresponding centroids.</li> </ol> Product Quantization <p>Let's see how it works in the implementation, of that we\u2019ll create a random array of size 12 and keep the chunk size as 3.</p> <p><pre><code>import random\n\n#consider this as a high dimensional vector\nvec = v = [random.randint(1,20)) for i in range(12)]chunk_count = 4\nvector_size = len(vec)\n\n# vector_size must be divisable by chunk_size\nassert vector_size % chunk_count == 0\n# length of each subvector will be vector_size/ chunk_count\nsubvector_size = int(vector_size / chunk_count)\n\n# subvectors\nsub_vectors = [vec[row: row+subvector_size] for row in range(0, vector_size, subvector_size)]\nsub_vectors\n</code></pre> The output looks like this: <pre><code>[[13, 3, 2], [5, 13, 5], [17, 8, 5], [3, 12, 9]]\n</code></pre></p> <p>These subvectors are substituted with a designated centroid vector called Reproduction Value because it helps identify each subvector. Subsequently, this centroid vector can be substituted with a distinct ID that is unique to it.</p> <pre><code>k = 2**5\nassert k % chunk_count == 0\nk_ = int(k/chunk_count)\n\nfrom random import randint\n# reproduction values\nc = []  \nfor j in range(chunk_count):\n    # each j represents a subvector position\n    c_j = []\n    for i in range(k_):\n        # each i represents a cluster/reproduction value position \n       c_ji = [randint(0, 9) for _ in range(subvector_size)]\n       c_j.append(c_ji)  # add cluster centroid to subspace list\n\n  # add subspace list of centroids\n    c.append(c_j)#helper function to calculate euclidean distance\ndef euclidean(v, u):\n    distance = sum((x - y) ** 2 for x, y in zip(v, u)) ** .5\n    return distance\n\n#helper function to create unique ids\ndef nearest(c_j, chunk_j):\n    distance = 9e9\n    for i in range(k_):\n        new_dist = euclidean(c_j[i], chunk_j)\n        if new_dist &lt; distance:\n            nearest_idx = i\n            distance = new_dist\n    return nearest_idx\n</code></pre> <p>Now, Let's see How we can get unique centroid IDs using the nearest helper function</p> <p><pre><code>ids = []\n# unique centroid IDs for each subvector\nfor j in range(chunk_count):\n    i = nearest(c[j], sub_vectors[j])\n    ids.append(i)\nprint(ids)\n</code></pre> Output shows unique centroid IDs for each subvector:</p> <p><pre><code>[5, 6, 7, 7]\n</code></pre> When utilizing PQ to handle a vector, we divide it into subvectors. These subvectors are then processed and linked to their closest centroids, also known as reproduction values, within the respective subclusters.</p> <p>Note</p> <p>Instead of saving our Quantized Vector using the centroids, we substitute it with a unique Centroid ID. Each centroid has its specific ID, allowing us to later map these ID values back to the complete centroids.</p> <p><pre><code>quantized = []\nfor j in range(chunk_count):\n    c_ji = c[j][ids[j]]\n    quantized.extend(c_ji)\n\nprint(quantized)\n</code></pre> Here is the reconstructed vector using Centroid IDs: <pre><code>[9, 9, 2, 5, 7, 6, 8, 3, 5, 2, 9, 4]\n</code></pre> In doing so, we\u2019ve condensed a 12-dimensional vector into a 4-dimensional vector represented by IDs. We opted for a smaller dimensionality for simplicity, which might make the advantages of this technique less immediately apparent.</p> <p>Note</p> <p>It\u2019s important to highlight that the reconstructed vector is not identical to the original vector. This discrepancy arises due to inherent losses during the compression and reconstruction process in all compression algorithms.</p> <p>Let\u2019s change our starting 12-dimensional vector made of 8-bit integers to a more practical 128-dimensional vector of 32-bit floats. By compressing it to an 8-bit integer vector with only eight dimensions, we strike a good balance in performance.</p> <p>Original: 128\u00d732 = 4096 Quantized: 8\u00d78 = 64</p> <p>This marks a substantial difference \u2014 a 64x reduction in memory.</p>"},{"location":"writing/2024/08/23/IVF_PQ-accelerate-vector-search-by-creating-indices/#how-ivfpq-index-help-in-speeding-things-up","title":"How IVFPQ Index help in speeding things up?","text":"<p>In IVFPQ, an Inverted File index (IVF) is integrated with Product Quantization (PQ) to facilitate a rapid and effective approximate nearest neighbor search by initial broad-stroke that narrows down the scope of vectors in our search.</p> <p>After this, we continue our PQ search as we did before \u2014 but with a significantly reduced number of vectors. By minimizing our Search Scope, it is anticipated to achieve significantly improved search speeds.</p> <p>IVFPQ can be very easily implemented in just a few lines of code using LanceDB</p> <p>Creating an IVF_PQ Index</p> <p><pre><code>import lancedb\nimport numpy as np\nuri = \"./lancedb\"\ndb = lancedb.connect(uri)\n\n# Create 10,000 sample vectors\ndata = [{\"vector\": row, \"item\": f\"item {i}\"}\n   for i, row in enumerate(np.random.random((10_000, 1536)).astype('float32'))]\n\n# Add the vectors to a table\ntbl = db.create_table(\"my_vectors\", data=data)\n\n# Create and train the index - you need to have enough data in the table for an effective training step\ntbl.create_index(num_partitions=256, num_sub_vectors=96)\n</code></pre> - metric (default: \u201cL2\u201d): The distance metric to use. By default, it uses Euclidean distance \u201cL2\". We also support \"cosine\" and \"dot\" distance as well. - num_partitions (default: 256): The number of partitions of the index. - num_sub_vectors (default: 96): The number of sub-vectors (M) that will be created during Product Quantization (PQ).</p> <p>Now let's see what this IVF Index does to reduce the scope of vectors, Inverted file is an index structure that is used to map database vectors to their respective partitions where these vectors reside.</p> PQ vectors Vectors are assigned to different Voronoi cells via IVF  <p>This is Voronoi's Representation of vectors using IVF, they\u2019re simply a set of partitions each containing vectors close to each other, and when it comes to search \u2014 When we introduce our query vector, it restricts our search to the nearest cells only because of which searching becomes way faster compared to PQ.</p> Query Vector searches Closest cell  <p>Afterwards, PQ needs to be applied as we have seen above.</p> <p>All of this can be applied using the IVF+PQ Index using LanceDB in minimal lines of code</p> <pre><code>tbl.search(np.random.random((1536))) \\\n    .limit(2) \\\n    .nprobes(20) \\\n    .refine_factor(10) \\\n    .to_pandas()\n</code></pre> <ul> <li>limit (default: 10): The number of results that will be returned</li> <li>n-probes (default: 20): The quantity of probes (sections) determines the distribution of vector space. While a higher number enhances search accuracy, it also results in slower performance. Typically, setting the number of probes (n-probes) to cover 5\u201310% of the dataset proves effective in achieving high recall with minimal latency.</li> <li>refine_factor (default: None): Refine the results by reading extra elements and re-ranking them in memory.</li> </ul> <p>A higher number makes the search more accurate but also slower.</p>"},{"location":"writing/2024/08/23/IVF_PQ-accelerate-vector-search-by-creating-indices/#conclusion","title":"Conclusion","text":"<p>In summary, Product Quantization helps reduce memory usage when storing high-dimensional vectors. Along with the IVF index, it significantly speeds up the search process by focusing only on the nearest vectors.</p>"},{"location":"writing/2024/08/13/cambrian-1-vision-centric-search/","title":"Cambrian-1: Vision-Centric Search","text":"<p>Cambrian-1 is a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can boost multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research.</p> <p></p> <p>Cambrian-1 is built on five key pillars, each providing important insights into the design of multimodal LLMs (MLLMs):</p> <ol> <li>Visual Representations: They explore various vision encoders and their combinations.</li> <li>Connector Design: They design a new dynamic, spatially-aware connector that integrates visual features from several models with LLMs while reducing the number of tokens.</li> <li>Instruction Tuning Data: They curate high-quality visual instruction-tuning data from public sources, emphasizing distribution balancing.</li> <li>Instruction Tuning Recipes: They discuss strategies and best practices for instruction tuning.</li> <li>Benchmarking: They examine existing MLLM benchmarks and introduce a new vision-centric benchmark called \"CV-Bench\".</li> </ol> <p>We'll learn how Cambrian-1 works with an example of Vision-Centric Exploration on images found through vector search. This will involve two steps. 1. Performing vector search to get related images 2. Use obtained images for vision-centric exploration</p> <p></p>"},{"location":"writing/2024/08/13/cambrian-1-vision-centric-search/#implementation","title":"Implementation","text":"<p>This blog contains code snippets, for the whole code with their description use checkout Kaggle Notebook and run it with your own prompt</p> <p>Kaggle Notebook</p> <p>Kaggle notebook demonstrating this tutorial</p> <p>https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/ </p> <p>In this example, We will be working with the\u00a0Flickr-8k\u00a0dataset. It is a multi-modal dataset comprising images and their corresponding captions.</p> <p>We'll index these images based on their captions using the all-mpnet-base-v2 model from the sentence-transformer. The LanceDB Embedding API will retrieve embeddings from the sentence transformer models. </p> <pre><code>embedding_model = (\n    get_registry()\n    .get(\"sentence-transformers\")\n    .create(name=\"all-mpnet-base-v2\", device=\"cuda:0\")\n)\n</code></pre> <p>Now we are ready to integrate this embedding function into the schema of the table to simplify the process of data ingestion and querying.</p> <pre><code>pa_schema = pa.schema([\n    pa.field(\"vector\", pa.list_(pa.float32(), 768)),\n    pa.field(\"image_id\", pa.string()),\n    pa.field(\"image\", pa.binary()),\n    pa.field(\"captions\",pa.string()),\n])\n\nclass Schema(LanceModel):\n    vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()\n    image_id: str\n    image: bytes\n    captions: str = embedding_model.SourceField()\n</code></pre>"},{"location":"writing/2024/08/13/cambrian-1-vision-centric-search/#ingestion-pipeline","title":"Ingestion Pipeline","text":"<p>Storing vector embeddings efficiently is crucial for leveraging their full potential in various applications. To manage, query, and retrieve embeddings effectively, especially with large-scale and multi-modal data, we need a robust solution. LanceDB allows you to store, manage, and query embeddings as well as raw data files.</p> <pre><code>db = lancedb.connect(\"dataset\")\ntbl = db.create_table(\"table\", schema=Schema, mode=\"overwrite\")\ntbl.add(process_dataset(df))\n</code></pre> <p>The above snippet populates a table that you can use to query using captions.</p>"},{"location":"writing/2024/08/13/cambrian-1-vision-centric-search/#vector-search","title":"Vector Search","text":"<p>With LanceDB, performing vector search is pretty straightforward. LanceDB Embedding API implicitly converts the query into embedding and performs vector search to give desired results. Let's take a look at the following example:</p> <pre><code>query = \"cat sitting on couch\"\n\nhit_lists = tbl.search(query) \\\n    .metric(\"cosine\") \\\n    .limit(2) \\\n    .to_list()\n\nfor hit in hit_lists:\n    show_image(hit[\"image\"])\n</code></pre> <p>This code sample returns the top two similar images, you can increase the number by changing <code>limit</code> parameter. </p> <p>The results would look like this</p> <p></p> <p></p> <p>Now we have obtained images from the vector search, let's explore these images with a vision-centric multi-modal LLM(MLLM) Cambrian-1.</p>"},{"location":"writing/2024/08/13/cambrian-1-vision-centric-search/#setup-vision-centric-exploration","title":"Setup vision-centric Exploration","text":"<p>We'll use Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\u00a0vision-centric\u00a0approach. For setting up Cambrian-1, we'll use their official GitHub repo https://github.com/cambrian-mllm/cambrian. </p> <p>Clone the repo and install requirements  <pre><code># clone \n!git clone https://github.com/cambrian-mllm/cambrian\n%cd cambrian\n\n# install gpu related requirements\n!pip install \".[gpu]\" -q\n</code></pre></p> <p>Kaggle Notebook</p> <p>To go through this notebook, run each cell one by one.</p> <p>https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/ </p> <pre><code>prompt = \"How many cats are in image and Why?\"\nimages_path = []\nfor hit in hit_lists:\n    image_path = f\"/kaggle/working/Images/{hit['image_id']}\"\n    images_path.append(image_path)\n\ninfer_cambrian(images_path, prompt)\n\nimport subprocess\ncommand = \"python3 inference.py\"\noutput = subprocess.check_output(command, shell=True, text=True)\n</code></pre> <p>This code snippet will run <code>inference.py</code> and return the query results corresponding to each input image.</p> <p>The result will look like this</p> <p></p> <p>Our Prompt for exploring images was <code>How many cats are in image and Why?</code> , and Cambrian-1 has clearly given the right and clear output stating the correct count and external scenario.</p>"},{"location":"writing/2024/08/13/cambrian-1-vision-centric-search/#conclusion","title":"Conclusion","text":"<p>In this tutorial, We've explored the field of vision-centric exploration same as GPT4-o where you give an image as input and ask questions related to it. We have utilized Cambrian-1 as a multi-modal LLM for vision-centric exploration combined with vector search using text embeddings.</p> <p>We've seen how LanceDB, with its powerful and elegant design in Embedding API, simplifies the process of storing, managing, and querying these embeddings, making sophisticated tasks like semantic search both accessible and efficient.</p>"},{"location":"writing/2024/10/07/contextual-retrieval/","title":"Improve RAG with Contextual Retrieval","text":"<p>For chatbots to be effective in domains like legal advice or customer support, they require relevant background information. Retrieval-Augmented Generation (RAG) enhances responses by pulling information from knowledge bases and combining it with the user\u2019s prompt, improving the model's output. However, traditional RAG often end up losing crucial context, leading to missing relevant information from the knowledge base. Some might suggest that writing very specific and lengthy context prompts could solve the issue of retrieving relevant information from a knowledge base. But this approach only works for smaller knowledge bases. You'll need a more efficient and scalable solution as your knowledge base expands.</p> <p></p>"},{"location":"writing/2024/10/07/contextual-retrieval/#contextual-retrieval","title":"Contextual Retrieval","text":"<p>In traditional RAG, a basic chunking method creates vector embeddings for each chunk separately, and RAG systems use these embeddings to find chunks that match the query. However, this approach has a problem: it loses the context of the original document.  In the past, several methods have been proposed to improve retrieval using context. These include adding generic document summaries to chunks,  *Hypothetical Document Embedding (HyDE)***, and summary-based indexing**.</p> <p>Contextual Embeddings address this issue by incorporating relevant context and prepending it into each chunk before creating embeddings. This approach enhances the quality of each embedded chunk, leading to more accurate retrieval and improved overall performance. On average, across all the data sources we tested, Contextual Embeddings reduced the failure rate for retrieving the top 20 chunks by 35%.Contextual Retrieval Processing</p> <p></p> <p>To understand it better with domain-specific examples</p> <p>In the case of  Legal Documents</p> <p>Scenario: A user asks, \"What was the outcome of the Johnson v. Smith case?\"</p> <p>Relevant Chunk: \"The court ruled in favor of Johnson, awarding damages of $50,000.\"</p> <p>Problem: Without additional context, it\u2019s unclear which Johnson and Smith are being referenced, or the date of the case, making it difficult to retrieve or apply the information.</p> <p>Solution: Contextual Retrieval can enhance the chunk by including key identifiers, such as \u201cIn the 2021 case of Johnson v. Smith in the New York District Court, the court ruled in favor of Johnson, awarding damages of $50,000.\u201d This added context helps ensure accurate retrieval and interpretation.</p> <p>In the case of Customer Support</p> <p>Scenario: A customer asks, \"What is the return policy for electronics?\"</p> <p>Relevant Chunk: \"Customers have 30 days to return items for a full refund.\"</p> <p>Problem: Without knowing that this applies specifically to electronics and any conditions (like whether the item must be unopened), the information may be incomplete.</p> <p>Solution: Contextual Retrieval can enhance the response by saying, \"For electronics, customers have 30 days to return items in their original packaging for a full refund.\" This adds necessary context that clarifies the policy.</p>"},{"location":"writing/2024/10/07/contextual-retrieval/#implementation","title":"Implementation","text":"<p>To implement Contextual Retrieval, we need to annotate thousands or even millions of chunks in a knowledge base by prepending context with them. We can do the same using Claude3 by creating a prompt that directs the model to provide concise, chunk-specific context by using the overall document's context to generate context for each chunk.  {{WHOLE_DOCUMENT}}    Here is the chunk we want to situate within the whole document    {{CHUNK_CONTENT}}    Please give a short succinct context to situate this chunk within the overall document to improve search retrieval of the chunk. Answer only with the succinct context and nothing else. </p> <p>The resulting contextual text, typically between 50-100 tokens, is added to the chunk as a context before embedding and indexing it. Let's take an example to understand how to generate the contextual chunks, which asks the Claude3-Haiku LLM to provide a succinct context for the chunk within the document. Following are the raw chunks utilized by Naive RAG:</p> <pre><code>The recent SEC filing provided insights into ACME Corp's performance for Q2 2023.\n----------------------------------------------------------------------------\nIt highlighted a 3% revenue growth over the previous quarter.\n----------------------------------------------------------------------------\nThe company, which had a revenue of $314 million in the prior quarter, showed steady progress.\n----------------------------------------------------------------------------\nThey attributed this growth to strategic initiatives and operational efficiencies.\n----------------------------------------------------------------------------\nThe report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects.\n----------------------------------------------------------------------------\n</code></pre> <p>Now we'll augment these chunks, which include the context of the document provided by the LLM and the chunk.</p> <pre><code>The document provides an overview of ACME Corp's financial performance in Q2 2023 based on their recent SEC filing. The recent SEC filing provided insights into ACME Corp's performance for Q2 2023.\n----------------------------------------------------------------------------------------------------\nThe document provides an overview of ACME Corp's financial performance in Q2 2023 based on the recent SEC filing. It highlighted a 3% revenue growth over the previous quarter.\n----------------------------------------------------------------------------------------------------\nThe document provides an overview of ACME Corp's financial performance in Q2 2023. The company, which had a revenue of $314 million in the prior quarter, showed steady progress.\n----------------------------------------------------------------------------------------------------\nThe document provides an overview of ACME Corp's financial performance for the second quarter of 2023 based on its recent SEC filing. They attributed this growth to strategic initiatives and operational efficiencies.\n----------------------------------------------------------------------------------------------------\nThe document provides an overview of ACME Corp's financial performance in Q2 2023 based on their recent SEC filing. The report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects.\n----------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"writing/2024/10/07/contextual-retrieval/#things-to-keep-in-mind","title":"Things to keep in mind","text":"<p>When implementing Contextual Retrieval, keep the following factors in mind: 1. Chunk Boundaries: How you split documents into chunks can affect retrieval performance. Pay attention to chunk size, boundaries, and overlap. 2. Embedding Model: While Contextual Retrieval improves performance across various models, some like Gemini and OpenAI may benefit more. 3. Custom Prompts: A generic prompt works well, but tailored prompts for your specific domain\u2014such as including a glossary\u2014can yield better results. 4. Number of Chunks: More chunks in the context window increase the chance of capturing relevant information, but too many can overwhelm the model. We found that using 20 chunks performed best, though it\u2019s good to experiment based on your needs.</p>"},{"location":"writing/2024/10/07/contextual-retrieval/#boost-up-contextual-retriever-with-reranking","title":"Boost up Contextual retriever with Reranking","text":"<p>In the final step, we can boost performance by combining Contextual Retrieval with Reranking. In traditional RAG, vector search often retrieves many chunks from the knowledge base \u2014 of varying relevance.</p> <p>In the context of search, Reranking means reordering the search results returned by a search algorithm based on some criteria. This can be useful when the initial ranking of the search results is not satisfactory or when the user has provided additional information that can be used to improve the ranking of the search results.</p> <p>Steps for performing Reranking : 1. Perform initial retrieval to get the top chunks. Pass these chunks, along with the user\u2019s query, to the reranking model. 2. Score each chunk based on relevance, then select the top chunks (we used the top 20). Input the top chunks into the model as context to generate the final result. LanceDB Reranking API LanceDB comes with some built-in rerankers.\u00a0To use a reranker, you need to create an instance of the reranker and pass it to the\u00a0<code>rerank</code>\u00a0method of the query builder.</p> <pre><code>from lancedb.rerankers import ColbertReranker\ncolbert = ColbertReranker()\n\ntable.search(\"query\").rerank(reranker=colbert) # reranker vector search\ntable.search(\"query\", query_type=\"fts\").rerank(reranker=colbert)\n</code></pre> <p>Learn more about Improving (almost) any retriever with LanceDB hybrid search and Reranking.</p>"},{"location":"writing/2024/10/07/contextual-retrieval/#conclusion","title":"Conclusion","text":"<p>Using the combinations of techniques and steps described \u2014 proper chunking, embedding models, Contextual Retrieval, and reranking with top-K results across various datasets\u2014here\u2019s a summary of our findings: 1. Hybrid search with BM25 outperforms vector search alone.</p> <ol> <li> <p>OpenAI and Gemini are among the best embedding models.</p> </li> <li> <p>Passing the top-20 to top-30 chunks to the model is more effective than using just the top-10 or top-5.</p> </li> <li> <p>Adding context to chunks significantly improves retrieval accuracy and enables Contextual Retrieval.</p> </li> <li> <p>LanceDB Reranking API yields better results than not using it.</p> </li> </ol>"},{"location":"writing/archive/2024/","title":"2024","text":""},{"location":"writing/category/rag/","title":"RAG","text":""},{"location":"writing/category/ann-indexing/","title":"ANN Indexing","text":""},{"location":"writing/category/multimodal-llm/","title":"Multimodal LLM","text":""}]}